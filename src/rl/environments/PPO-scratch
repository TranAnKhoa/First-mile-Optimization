import gymnasium as gym
from gymnasium import spaces
import numpy as np
import copy
import math
import random
import json
import os
import sys

# ==============================================================================
# 1. FIX ÄÆ¯á»œNG DáºªN & IMPORT
# ==============================================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.abspath(os.path.join(current_dir, '..', '..'))
if src_path not in sys.path: sys.path.insert(0, src_path)

try:
    from routing.cvrp.alns_cvrp.cvrp_env import cvrpEnv
    from routing.cvrp.alns_cvrp.initial_solution import compute_initial_solution
    
    from routing.cvrp.alns_cvrp.destroy_operators import (
        random_removal, worst_removal_alpha_0, worst_removal_bigM, 
        worst_removal_adaptive, time_worst_removal, shaw_spatial, 
        shaw_hybrid, shaw_temporal, shaw_structural, trip_removal, 
        historical_removal, update_solution_state_after_destroy
    )
    
    from routing.cvrp.alns_cvrp.repair_operators import (
        best_insertion, regret_2_position, regret_2_trip, regret_2_vehicle, 
        regret_3_position, regret_3_trip, regret_3_vehicle, 
        regret_4_position, regret_4_trip, regret_4_vehicle
    )
    
    from routing.cvrp.alns_cvrp.utils import (
        optimize_all_start_times, update_history_matrix, cleanup_inter_factory_routes
    )
except ImportError as e:
    print(f"âŒ [Env] Lá»—i Import: {e}")
    raise e

# ==============================================================================
# 2. Cáº¤U HÃŒNH TOÃN Tá»¬
# ==============================================================================
DESTROY_OPS = [random_removal, worst_removal_alpha_0, worst_removal_bigM, worst_removal_adaptive, time_worst_removal, shaw_spatial, shaw_hybrid, shaw_temporal, shaw_structural, trip_removal, historical_removal]
REPAIR_OPS = [best_insertion, regret_2_position, regret_2_trip, regret_2_vehicle, regret_3_position, regret_3_trip, regret_3_vehicle, regret_4_position, regret_4_trip, regret_4_vehicle]
REMOVE_LEVELS = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]

def get_op_name(op):
    if hasattr(op, '__name__'): return op.__name__
    if hasattr(op, 'func'): return op.func.__name__
    return str(op)

# ==============================================================================
# 3. CLASS PPO ALNS (MACRO-OP VERSION)
# ==============================================================================
class PPO_ALNS_Env_GP(gym.Env):
    def __init__(self, problem_instance, max_iterations=200, buffer_size=1, **kwargs):
        super(PPO_ALNS_Env_GP, self).__init__()
        
        self.problem_instance = problem_instance
        self.random_state = np.random.RandomState()
        
        # --- LOAD TUYá»†T Ká»¸ Tá»ª JSON ---
        json_filename = 'macro_advanced_safety.json'
        # TÃ¬m file á»Ÿ thÆ° má»¥c hiá»‡n táº¡i hoáº·c root
        json_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), json_filename)
        if not os.path.exists(json_path): 
            json_path = json_filename # Thá»­ tÃ¬m á»Ÿ root

        if os.path.exists(json_path):
            with open(json_path, 'r') as f:
                self.macro_ops = json.load(f)
            print(f"âœ… [Env] Loaded {len(self.macro_ops)} Macro-Operators.")
        else:
            print(f"âš ï¸ [Env] Warning: '{json_filename}' not found. Using dummy mode.")
            self.macro_ops = []

        # --- ACTION SPACE: Chá»n 1 trong N Tuyá»‡t ká»¹ ---
        self.num_actions = len(self.macro_ops) if self.macro_ops else 10
        self.action_space = spaces.Discrete(self.num_actions)
        
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32)
        self.max_iterations = max_iterations
        self.buffer_size = buffer_size # LuÃ´n lÃ  1 á»Ÿ cháº¿ Ä‘á»™ nÃ y

        self.current_solution = None
        self.best_solution = None
        self.history_matrix = {}
        
        # Init cÃ¡c biáº¿n theo dÃµi
        self.best_objective = float('inf')
        self.initial_objective = float('inf')
        self.stag_count = 0
        self.current_iteration = 0

    # --- HÃ€M Cáº¤P Cá»¨U (SAFETY NET) ---
    def _sanitize_and_repair(self, solution):
        sol = cleanup_inter_factory_routes(solution)
        _, time_pen, _, cap_pen = sol.objective()
        
        if time_pen == 0 and cap_pen == 0:
            return sol

        # DÃ¹ng TimeWorst Ä‘á»ƒ sá»­a lá»—i (khÃ´ng in log Ä‘á»ƒ cháº¡y nhanh)
        destroy_op = time_worst_removal
        op_kwargs = {'remove_fraction': 0.10, 'history_matrix': self.history_matrix}
        
        destroyed, unvisited = destroy_op(sol, self.random_state, **op_kwargs)
        destroyed = update_solution_state_after_destroy(destroyed)
        
        if unvisited:
            farms = [c for c in unvisited if not str(c).startswith('TRANSFER_')]
            if farms:
                repaired, _ = regret_3_trip(destroyed, self.random_state, unvisited_customers=farms)
                return repaired
        return destroyed

    # --- HÃ€M THá»°C THI TUYá»†T Ká»¸ ---
    def _execute_macro_op(self, op_index, solution):
        current_temp = copy.deepcopy(solution)
        
        if not self.macro_ops: return current_temp 
            
        op_data = self.macro_ops[op_index]
        sequence_indices = op_data['sequence_indices'] 
        op_kwargs = {'history_matrix': self.history_matrix}

        for i, step_indices in enumerate(sequence_indices):
            d_idx, p_idx, r_idx = step_indices
            
            # Safety Net cho chuá»—i dÃ i
            if i >= 2: current_temp = self._sanitize_and_repair(current_temp)

            try:
                d_op = DESTROY_OPS[d_idx]
                op_kwargs['remove_fraction'] = REMOVE_LEVELS[p_idx]
                r_op = REPAIR_OPS[r_idx]
                
                current_temp = cleanup_inter_factory_routes(current_temp)
                destroyed, unvisited = d_op(current_temp, self.random_state, **op_kwargs)
                destroyed = update_solution_state_after_destroy(destroyed)
                
                if unvisited:
                    farms = [c for c in unvisited if not str(c).startswith('TRANSFER_')]
                    if farms:
                        repaired, _ = r_op(destroyed, self.random_state, unvisited_customers=farms)
                        current_temp = repaired
                    else: current_temp = destroyed
                else: current_temp = destroyed
            except:
                return solution

        # Final check
        current_temp = self._sanitize_and_repair(current_temp)
        current_temp = optimize_all_start_times(current_temp)
        return current_temp

    # --- GYM INTERFACE ---
    def reset(self, seed=None, options=None):
        if seed is not None: self.random_state = np.random.RandomState(seed)
        
        # Fix lá»—i seed cho cvrpEnv
        sim_seed = self.random_state.randint(0, 1000000)
        initial_schedule = compute_initial_solution(self.problem_instance, self.random_state)
        
        self.initial_solution = cvrpEnv(initial_schedule, self.problem_instance, seed=sim_seed)
        self.initial_solution = cleanup_inter_factory_routes(self.initial_solution)
        
        self.current_solution = copy.deepcopy(self.initial_solution)
        self.best_solution = copy.deepcopy(self.initial_solution)
        self.history_matrix = {}
        update_history_matrix(self.history_matrix, self.current_solution)
        
        metrics = self.initial_solution.objective()
        self.initial_objective = metrics[0]
        self.best_objective = metrics[0]
        self.stag_count = 0
        self.current_iteration = 0
        
        return self._get_state(), {}

    def _get_state(self):
        metrics = self.current_solution.objective()
        current_obj = metrics[0]
        time_penalty = metrics[1] if len(metrics) > 1 else 0
        wait_time = metrics[2] if len(metrics) > 2 else 0
        cap_penalty = metrics[3] if len(metrics) > 3 else 0
        
        epsilon = 1e-6
        progress = self.current_iteration / self.max_iterations
        current_temp = (self.initial_objective * 0.05) * (1 - progress)
        
        state = np.array([
            (current_obj - self.best_objective) / (self.best_objective + epsilon),
            self.stag_count / ((self.max_iterations / 10) + epsilon),
            progress,
            current_temp / (self.initial_objective + epsilon),
            current_obj / (self.initial_objective + epsilon),
            time_penalty / (current_obj + epsilon),
            cap_penalty / (current_obj + epsilon),
            wait_time / (current_obj + epsilon),
            len(self.current_solution.schedule) / (len(self.initial_solution.schedule) + epsilon)
        ], dtype=np.float32)
        return state

    def step(self, action):
        # [QUAN TRá»ŒNG]: Action lÃ  1 sá»‘ nguyÃªn (Index cá»§a Tuyá»‡t ká»¹)
        op_index = int(action) 
        self.current_iteration += 1
        
        objective_before = self.current_solution.objective()[0]
        
        # 1. Thá»±c thi Tuyá»‡t ká»¹
        new_solution = self._execute_macro_op(op_index, self.current_solution)
        update_history_matrix(self.history_matrix, new_solution)
        
        final_results = new_solution.objective()
        objective_after = final_results[0]
        
        # 2. TÃ­nh Reward & Acceptance
        improvement = (objective_before - objective_after) / (objective_before + 1e-6)
        reward = improvement * 10 
        
        accepted = False
        is_new_best = False
        
        if objective_after < objective_before:
            accepted = True
        else:
            progress = self.current_iteration / self.max_iterations
            current_temp = (self.initial_objective * 0.05) * (1 - progress)
            current_temp = max(current_temp, 1e-6)
            diff = objective_after - objective_before
            
            if diff > self.initial_objective * 0.5: probability = 0
            else: probability = math.exp(-diff / current_temp)
                
            if self.random_state.rand() < probability: accepted = True

        # 3. Cáº­p nháº­t
        if accepted:
            self.current_solution = new_solution
            if objective_after < self.best_objective:
                self.best_objective = objective_after
                self.best_solution = copy.deepcopy(new_solution)
                is_new_best = True
                # Log nháº¹ Ä‘á»ƒ biáº¿t cÃ³ tiáº¿n triá»ƒn
                # print(f"ðŸŽ‰ New Best: {self.best_objective:.2f} (Op #{op_index})")

        if is_new_best:
            self.stag_count = 0
            reward += 20.0 
        else:
            self.stag_count += 1
            if not accepted: reward -= 0.5
            elif improvement <= 0: reward -= 0.1

        done = self.current_iteration >= self.max_iterations
        info = {'best_objective': self.best_objective}

        return self._get_state(), reward, done, False, info