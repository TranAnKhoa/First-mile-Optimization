import gym
from gym import spaces
import numpy as np
import copy
import math
import random
from deap import base, creator, tools

# ==============================================================================
# L·ªöP GPSequence (T·ª´ m√£ ngu·ªìn c·ªßa b·∫°n)
# ==============================================================================

class GPSequence:
    def __init__(self, pairs, fitness_fn, ngen=10, pop_size=10):
        self.pairs = pairs 
        self.fitness_fn = fitness_fn
        self.ngen = ngen
        self.pop_size = pop_size

        if not hasattr(creator, "FitnessMin"): # S·ª≠a th√†nh FitnessMin v√¨ objective c√†ng th·∫•p c√†ng t·ªët
            creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
        if not hasattr(creator, "Individual"):
            creator.create("Individual", list, fitness=creator.FitnessMin)

        self.toolbox = base.Toolbox()
        self.toolbox.register("indices", random.sample, range(len(self.pairs)), len(self.pairs))
        self.toolbox.register("individual", tools.initIterate, creator.Individual, self.toolbox.indices)
        self.toolbox.register("population", tools.initRepeat, list, self.toolbox.individual)
        self.toolbox.register("mate", tools.cxOrdered)
        self.toolbox.register("mutate", tools.mutShuffleIndexes, indpb=0.2)
        
        def eval_func(individual):
            sequence = [self.pairs[i] for i in individual]
            return (self.fitness_fn(sequence),)
        self.toolbox.register("evaluate", eval_func)

    def run(self):
        pop = self.toolbox.population(n=self.pop_size)
        
        # Thu·∫≠t to√°n di truy·ªÅn ƒë∆°n gi·∫£n (b·∫°n c√≥ th·ªÉ c·∫£i thi·ªán ph·∫ßn n√†y)
        for gen in range(self.ngen):
            offspring = tools.selTournament(pop, len(pop), tournsize=3)
            offspring = list(map(self.toolbox.clone, offspring))

            for child1, child2 in zip(offspring[::2], offspring[1::2]):
                if random.random() < 0.7:
                    self.toolbox.mate(child1, child2)
                    del child1.fitness.values, child2.fitness.values
            
            for mutant in offspring:
                if random.random() < 0.2:
                    self.toolbox.mutate(mutant)
                    del mutant.fitness.values
            
            invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
            fitnesses = self.toolbox.map(self.toolbox.evaluate, invalid_ind)
            for ind, fit in zip(invalid_ind, fitnesses):
                ind.fitness.values = fit

            pop[:] = offspring

        best_ind = tools.selBest(pop, 1)[0]
        best_seq = [self.pairs[i] for i in best_ind]
        return best_seq

# ==============================================================================
# PH·∫¶N IMPORT MODULE ALNS C·ª¶A B·∫†N
# ==============================================================================
try:
    from routing.cvrp.alns_cvrp.cvrp_env import cvrpEnv
    from routing.cvrp.alns_cvrp.initial_solution import compute_initial_solution
    from routing.cvrp.alns_cvrp.destroy_operators import random_removal, worst_removal, shaw_removal, time_worst_removal
    from routing.cvrp.alns_cvrp.repair_operators import best_insertion, regret_2_insertion, regret_3_insertion, regret_4_insertion
    print("‚úÖ ƒê√£ import th√†nh c√¥ng c√°c module ALNS c·ªßa b·∫°n!")
except ImportError:
    print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y module ALNS. S·ª≠ d·ª•ng c√°c l·ªõp gi·∫£ (dummy classes).")
    class cvrpEnv: pass
    def compute_initial_solution(problem, rand): return cvrpEnv()
    def random_removal(c, r, **k): return c, []
    def worst_removal(c, r, **k): return c, []
    def shaw_removal(c, r, **k): return c, []
    def time_worst_removal(c, r, **k): return c, []
    def best_insertion(c, r, **k): return c, []
    def regret_2_insertion(c, r, **k): return c, []
    def regret_3_insertion(c, r, **k): return c, []
    def regret_4_insertion(c, r, **k): return c, []

# ==============================================================================
# M√îI TR∆Ø·ªúNG PPO T√çCH H·ª¢P GP
# ==============================================================================
class PPO_ALNS_Env_GP(gym.Env):
    def __init__(self, problem_instance, max_iterations=125, buffer_size=8, **kwargs): # max_iter = 1000 / 8
        super(PPO_ALNS_Env_GP, self).__init__()
        
        self.problem_instance = problem_instance
        self.random_state = np.random.RandomState()
        self.destroy_operators = [random_removal, worst_removal, shaw_removal, time_worst_removal]
        self.repair_operators = [best_insertion, regret_2_insertion, regret_3_insertion, regret_4_insertion]
        
        self.action_space = spaces.MultiDiscrete([len(self.destroy_operators), len(self.repair_operators)])
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32)

        self.max_iterations = max_iterations # S·ªë l·∫ßn GP ƒë∆∞·ª£c g·ªçi
        self.buffer_size = buffer_size
        self.action_buffer = []

        # C√°c bi·∫øn theo d√µi kh√°c
        self.current_solution = None
        self.initial_solution = None
        self.best_solution = None
        self.initial_objective = float('inf')
        self.best_objective = float('inf')
        self.stag_count = 0
        self.current_iteration = 0
        self.start_temperature = kwargs.get('start_temperature', 1) # Kh√¥ng d√πng SA n√™n temp ch·ªâ ƒë·ªÉ l√†m feature

    def _fitness_function_for_gp(self, sequence):
        """
        H√†m m√¥ ph·ªèng th·ª±c thi m·ªôt chu·ªói c√°c to√°n t·ª≠ v√† tr·∫£ v·ªÅ objective.
        C√†ng th·∫•p c√†ng t·ªët.
        """
        temp_solution = copy.deepcopy(self.current_solution)
        for destroy_op, repair_op in sequence:
            destroyed, unvisited = destroy_op(temp_solution, self.random_state)
            if unvisited:
                repaired, _ = repair_op(destroyed, self.random_state, unvisited_customers=unvisited)
                temp_solution = repaired
        
        return temp_solution.objective()[0]

    def reset(self):
        print(">>> M√¥i tr∆∞·ªùng ƒë∆∞·ª£c reset. T·∫°o l·ªùi gi·∫£i ban ƒë·∫ßu m·ªõi...")
        initial_schedule = compute_initial_solution(self.problem_instance, self.random_state)
        self.initial_solution = cvrpEnv(initial_schedule, self.problem_instance, seed=None)
        self.current_solution = copy.deepcopy(self.initial_solution)
        self.best_solution = copy.deepcopy(self.initial_solution)
        
        initial_results = self.initial_solution.objective()
        self.initial_objective = initial_results[0]
        self.best_objective = initial_results[0]
        
        self.stag_count = 0
        self.current_iteration = 0
        self.action_buffer = []
        
        return self._get_state()

    def _get_state(self):
        # (Gi·ªØ nguy√™n h√†m _get_state ƒë√£ vi·∫øt ·ªü l·∫ßn tr∆∞·ªõc)
        current_metrics = self.current_solution.objective()
        current_obj, time_penalty, wait_time, cap_penalty = current_metrics[:4]
        epsilon = 1e-6

        state = np.array([
            (current_obj - self.best_objective) / (self.best_objective + epsilon),
            self.stag_count / ((self.max_iterations / 10) + epsilon),
            self.current_iteration / self.max_iterations,
            (self.start_temperature * (0.999 ** (self.current_iteration * self.buffer_size))) / self.start_temperature,
            current_obj / (self.initial_objective + epsilon),
            time_penalty / (current_obj + epsilon),
            cap_penalty / (current_obj + epsilon),
            wait_time / (current_obj + epsilon),
            len(self.current_solution.schedule) / (len(self.initial_solution.schedule) + epsilon)
        ], dtype=np.float32)
        
        return state

    def step(self, action):
        destroy_idx, repair_idx = action
        destroy_op = self.destroy_operators[destroy_idx]
        repair_op = self.repair_operators[repair_idx]
        self.action_buffer.append((destroy_op, repair_op))

        # --- KI·ªÇM TRA B·ªò ƒê·ªÜM ---
        if len(self.action_buffer) < self.buffer_size:
            # N·∫øu b·ªô ƒë·ªám ch∆∞a ƒë·∫ßy, kh√¥ng l√†m g√¨ c·∫£, ch·ªù b∆∞·ªõc ti·∫øp theo
            # Tr·∫£ v·ªÅ reward = 0 v√† state kh√¥ng ƒë·ªïi
            done = self.current_iteration >= self.max_iterations
            return self._get_state(), 0, done, {'best_objective': self.best_objective}

        # --- B·ªò ƒê·ªÜM ƒê√É ƒê·∫¶Y -> G·ªåI GP V√Ä TH·ª∞C THI ---
        self.current_iteration += 1
        print(f"\n--- Buffer ƒë·∫ßy. Iter {self.current_iteration}/{self.max_iterations}. G·ªçi GPSequence... ---")

        # 1. G·ªçi GP ƒë·ªÉ t√¨m th·ª© t·ª± t·ªët nh·∫•t
        gp = GPSequence(self.action_buffer, self._fitness_function_for_gp)
        best_sequence = gp.run()
        
        # 2. Th·ª±c thi chu·ªói t·ªëi ∆∞u
        objective_before = self.current_solution.objective()[0]
        
        new_solution = copy.deepcopy(self.current_solution)
        for destroy_op, repair_op in best_sequence:
            destroyed, unvisited = destroy_op(new_solution, self.random_state)
            if unvisited:
                repaired, _ = repair_op(destroyed, self.random_state, unvisited_customers=unvisited)
                new_solution = repaired
        
        objective_after = new_solution.objective()[0]
        
        # 3. T√≠nh to√°n Reward (cho c·∫£ chu·ªói 8 b∆∞·ªõc)
        reward = 0.0
        epsilon = 1e-6
        improvement = (objective_before - objective_after) / (objective_before + epsilon)
        reward += improvement * 10
        
        if objective_after < self.best_objective:
            reward += 10.0
            print(f"üéâ New best found! Obj: {self.best_objective:.2f} -> {objective_after:.2f}")
            self.best_objective = objective_after
            self.best_solution = copy.deepcopy(new_solution)
            self.stag_count = 0
        else:
            self.stag_count += 1
        
        if abs(improvement) < epsilon:
            reward -= 0.1

        # 4. C·∫≠p nh·∫≠t tr·∫°ng th√°i v√† reset buffer
        self.current_solution = new_solution
        self.action_buffer = [] # Quan tr·ªçng: X√≥a buffer ƒë·ªÉ b·∫Øt ƒë·∫ßu l·∫°i

        # 5. Tr·∫£ v·ªÅ k·∫øt qu·∫£
        done = self.current_iteration >= self.max_iterations
        next_state = self._get_state()
        info = {'best_objective': self.best_objective}

        return next_state, reward, done, info

    def render(self, mode='human'):
        print(
            f"Iter (GP calls): {self.current_iteration}/{self.max_iterations} | "
            f"Buffer size: {len(self.action_buffer)}/{self.buffer_size} | "
            f"Current Obj: {self.current_solution.objective()[0]:.2f} | "
            f"Best Obj: {self.best_objective:.2f} | "
            f"Stag: {self.stag_count}"
        )